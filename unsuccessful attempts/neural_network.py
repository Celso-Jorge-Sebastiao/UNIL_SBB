# -*- coding: utf-8 -*-
"""Neural Network

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Yw-zXgqWgJBx4vAME-BvI4sJUogp6Ka2
"""

# Import required packages
!pip install -U spacy==3.7.0
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import confusion_matrix
from sklearn.feature_extraction.text import CountVectorizer
import spacy

url = data=pd.read_csv("https://raw.githubusercontent.com/Celso-Jorge-Sebastiao/UNIL_SBB/main/data/training_data.csv")
df_test = pd.read_csv("https://raw.githubusercontent.com/Celso-Jorge-Sebastiao/UNIL_SBB/main/data/unlabelled_test_data.csv")
df = pd.read_csv(url)
df

X = df.drop(columns='difficulty')
y = df[['difficulty']]
encoder = OneHotEncoder(sparse=False)
encoded_data = encoder.fit_transform(y[['difficulty']])
column_names = encoder.get_feature_names_out(input_features=['difficulty'])
y = pd.DataFrame(encoded_data, columns=column_names)

# Using default tokenizer
import re
from spacy.lang.fr.stop_words import STOP_WORDS
stopwords = ["le", "la", "de", "et", "un", "une", "des","les","en","du","je","tu","il","elle","nous","vous","ils","elles","pour","dans","que","est","qui","par","sur","qu"]
# Import stopwords from English language
spacy_stopwords = spacy.lang.fr.stop_words.STOP_WORDS

# Print total number of stopwords
list_stopwords = list(spacy_stopwords)
# Print 20 stopwords
count = CountVectorizer(ngram_range=(1,1))

texts = df["sentence"].tolist()
ids = df["id"].tolist()

# Learn the vocabulary dictionary and return document-term matrix
bow = count.fit_transform(texts)
# Get feature names
feature_names = count.get_feature_names_out()
# Show as a dataframe
pd.set_option("display.max_columns", None)
X = pd.DataFrame(bow.todense(),columns=feature_names,index= ids)
sum_values = X.sum()

# Trier les résultats pour identifier les colonnes avec la somme la plus grande
sorted_columns = sum_values.sort_values(ascending=False)

# Afficher les résultats
print("Colonnes avec la somme la plus grande :")
print(sorted_columns[:10])

#Your code here:
X_train, X_test, y_train, y_test = train_test_split(X,y,shuffle=True,test_size=0.10)

X_train = torch.tensor(X_train.values, dtype=torch.float)
y_train = torch.tensor(y_train.values, dtype=torch.float)
X_test = torch.tensor(X_test.values, dtype=torch.float)
y_test = torch.tensor(y_test.values, dtype=torch.float)

# Define a neural network class here:
class Net(nn.Module):
    def __init__(self, D_in, H1, D_out):
        super(Net, self).__init__()

        self.linear1 = nn.Linear(D_in, H1)        # Linear transformation for hidden layer
        self.linear2 = nn.Linear(H1, D_out)       # Linear transformation for output layer
        self.activation = nn.ReLU()               # Activation function for hidden layer

    def forward(self, x):
        y_pred = self.activation(self.linear1(x))   # Hidden layer: linear transformation + ReLU
        y_pred = self.linear2(y_pred)               # Output layer: linear transformation
        return y_pred

D_in, D_out = X_train.shape[1], y_train.shape[1]
LR,repet,neuro = 0.001,125,100
model1 = Net(D_in, neuro, D_out)
# calculate how many parameters are in the model
#pytorch_total_params = sum(p.numel() for p in model1.parameters() if p.requires_grad)
#print(pytorch_total_params)

#criterion = nn.CrossEntropyLoss()
criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model1.parameters(), lr=LR)

# Perform your iterations here
losses1 = []
losses1_test = []

for t in range(repet):                # 500 iterations

    # Forward pass: compute prediction on training set
    y_pred = model1(X_train)

    # Compute loss
    loss = criterion(y_pred, y_train) #target = y_train
    #print(t, loss.item())
    losses1.append(loss.item())
    if torch.isnan(loss):
        break

    # Compute gradient
    optimizer.zero_grad()
    loss.backward()

    # Update
    optimizer.step()

    # Compute loss on test set
    losses1_test.append(criterion(model1(X_test), y_test).item())

    # Plot training and test loss
plt.figure(figsize=(6, 4))
plt.plot(losses1, label="Training loss")
plt.plot(losses1_test, label="Test loss")
plt.title('Evolution of training and test loss - 500 neurons')
plt.legend()
plt.show()









model1.eval()

# Effectuer des prédictions sur l'ensemble de test
with torch.no_grad():
    y_pred_test = model1(X_test)

# Obtenir les classes prédites en prenant l'indice de la classe ayant la probabilité la plus élevée
predicted_classes = torch.argmax(y_pred_test, dim=1)
true_classes = torch.argmax(y_test, dim=1)
correct_predictions = (predicted_classes == true_classes).sum().item()
total_samples = y_test.size(0)

# Calculer la précision
accuracy = correct_predictions / total_samples
print(f'Précision : {100 * accuracy:.2f}%')
conf_matrix = confusion_matrix(true_classes.detach().numpy(), predicted_classes.detach().numpy())
print(conf_matrix)
#test_values = true_classes.detach().numpy()
#test_predict = predicted_classes.detach().numpy()
#testing = pd.DataFrame(test_values, index = list(range(0,960)))
#testing["predict"] = test_predict
#testing["true"] = test_values
#testing = testing[["predict","true"]]

#print(testing["true"].value_counts())
#print(testing["predict"].value_counts())

#now with full training and the unlabelled test set

X_train = torch.tensor(X.values, dtype=torch.float)
y_train = torch.tensor(y.values, dtype=torch.float)
D_in, D_out = X_train.shape[1], y_train.shape[1]
#modify iterations
model2 = Net(D_in, neuro, D_out)
# calculate how many parameters are in the model
pytorch_total_params = sum(p.numel() for p in model2.parameters() if p.requires_grad)
print(pytorch_total_params)

# Define your MSE loss here:

criterion_test = nn.BCEWithLogitsLoss()
optimizer_test = torch.optim.Adam(model2.parameters(), lr=LR)
# Define your Adam optimizer for finding the weights of the network here

# Perform your iterations here
losses1 = []
losses1_test = []

for t in range(repet):                # 500 iterations

    # Forward pass: compute prediction on training set
    y_pred = model2(X_train)

    # Compute loss
    loss = criterion_test(y_pred, y_train)
    #print(t, loss.item())
    losses1.append(loss.item())
    if torch.isnan(loss):
        break

    # Compute gradient
    optimizer_test.zero_grad()
    loss.backward()

    # Update
    optimizer_test.step()

model2.eval()
texts_test = df_test["sentence"].tolist()
ids_test = df_test["id"].tolist()

# Learn the vocabulary dictionary and return document-term matrix
bow = count.transform(texts_test)
# Get feature names
feature_names = count.get_feature_names_out()
# Show as a dataframe
pd.set_option("display.max_columns", None)
X_df_test = pd.DataFrame(bow.todense(),columns=feature_names,index= ids_test)
X_df_test = torch.tensor(X_df_test.values, dtype=torch.float)

df_test_pred = model2(X_df_test)
predicted_classes = torch.argmax(df_test_pred, dim=1)
pred = predicted_classes.detach().numpy()
df_test['difficulty'] = pred

df_test["difficulty"] = df_test['difficulty'].replace({4: "C1", 0: "A1",5: "C2",1 :"A2",2 : "B1",3 : "B2"})

submission = df_test[["id","difficulty"]]
submission.to_csv('submission.csv', index=False)
from google.colab import files
# Assuming you have a file named 'predictions.csv'
files.download('submission.csv')



